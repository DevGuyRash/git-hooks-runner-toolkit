{
  "id": "snapshot_1761403989604_ea4mpf7i0",
  "approvalId": "approval_1761403989596_8do5ree0v",
  "approvalTitle": "Approve testing-audit requirements",
  "version": 1,
  "timestamp": "2025-10-25T14:53:09.604Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nCreate a dedicated testing-audit initiative that reproduces the recent Ephemeral Mode anomaly (overlay roots logging truncated paths) and expands end-to-end coverage for every installer entry point, subcommand, and flag. The work should catalogue all observed failures and gaps so follow-up fixes can be planned with clear evidence.\n\n## Alignment with Product Vision\n\nSupports the toolkit goal of providing dependable, policy-friendly Git hook automation by hardening the verification story for install flows, overlay precedence, and hook management behaviours.\n\n## Requirements\n\n### Requirement 1 — Ephemeral install verification matrix\n\n**User Story:** As a toolkit maintainer, I want a comprehensive matrix of Ephemeral Mode install scenarios so that truncated overlay logs and similar regressions are documented with reproducible steps.\n\n#### Acceptance Criteria\n\n1. WHEN `install.sh install --mode ephemeral` is executed against a clean repo in both vendored and shared-checkout contexts THEN the audit SHALL record hooks-path, overlay roots, and any mismatches (including truncated log output) with reproduction commands.\n2. IF Ephemeral Mode overlay resolution is invoked with each precedence (`ephemeral-first`, `versioned-first`, `merge`) THEN the audit SHALL capture the logged order alongside filesystem assertions for each root.\n3. WHEN uninstall and reinstall cycles are exercised with custom `core.hooksPath` values set beforehand THEN the audit SHALL document whether manifest restoration and log output stay consistent across runs.\n\n### Requirement 2 — CLI subcommand and flag coverage inventory\n\n**User Story:** As a release steward, I want an inventory of every CLI subcommand and option along with its automated coverage status so that we can prioritise missing or flaky cases.\n\n#### Acceptance Criteria\n\n1. WHEN reviewing `install`, `uninstall`, `stage`, `hooks`, and `config` commands THEN the audit SHALL enumerate all documented flags, note existing automated tests, and highlight gaps or behaviour changes observed during manual execution.\n2. IF a flag or subcommand lacks automated verification THEN the audit SHALL include a failing or missing-test entry with suggested assertions or fixtures to add.\n3. WHEN legacy aliases (`init`, `add`, `remove`) are invoked under modern flows THEN the audit SHALL explain current behaviour, coverage status, and any divergences versus expected output.\n\n### Requirement 3 — Test suite robustness and observability review\n\n**User Story:** As a quality engineer, I want actionable notes on improving the test harness so that failures like the overlay log truncation surface early with clear diagnostics.\n\n#### Acceptance Criteria\n\n1. WHEN auditing existing Bats and shell test helpers THEN the audit SHALL list brittleness points (environment dependencies, flaky setup, insufficient assertions) and proposed mitigations.\n2. IF additional logging or fixtures are required to capture overlay state, manifest contents, or CLI output THEN the audit SHALL describe these needs with concrete locations in the repo.\n3. WHEN summarising findings THEN the document SHALL recommend specific follow-up fixes or new tests to be scheduled as implementation tasks in the subsequent phase.\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n- Maintain separation between audit documentation, helper utilities, and future implementation changes so subsequent fixes can target scoped modules.\n- Ensure any proposed helper updates preserve the single-responsibility pattern used across `lib/` and `tests/helpers/`.\n\n### Performance\n- Audited test expansions SHOULD avoid introducing excessive runtime (target: end-to-end suite completes within current CI budget plus 10%).\n\n### Security\n- Audit steps MUST avoid exposing user-specific paths or secrets in committed artefacts; redact sensitive data when capturing logs.\n\n### Reliability\n- Findings MUST emphasise deterministic reproduction steps and required assertions so future fixes can be validated consistently across environments.\n\n### Usability\n- The resulting audit document SHOULD be consumable by contributors new to the toolkit, with clear command snippets and references to relevant scripts.\n",
  "fileStats": {
    "size": 4263,
    "lines": 60,
    "lastModified": "2025-10-25T14:52:55.225Z"
  },
  "comments": []
}